{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a0c10a1-6505-4349-9b38-4393f66a44b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Notice!!! Please use search function to check all the cells with \"notice\" keywords, those are sanity checks and potential manual variable changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c0972de-89be-4d8c-a023-b4dd9f9db9f5",
     "showTitle": true,
     "title": "Load packages & set variables"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################################\n",
    "## Import all necessary Packages\n",
    "##################################\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, Column, DataFrameNaFunctions, DataFrameStatFunctions, GroupedData, Row\n",
    "from pyspark.sql import functions as F  \n",
    "from pyspark.sql.functions import concat, col, lower, mean, bround, when, unix_timestamp, split\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "## register options\n",
    "# pd.set_option('max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#########################################################################################################\n",
    "## Please define the target Timestamp for the end date for the generation of COVID patients dataset\n",
    "## Notice: Format: YYYY-MM-DD\n",
    "## Current set to 2022-01-14\n",
    "############################################################################################################\n",
    "\n",
    "## check to make sure they are consistent\n",
    "## previous file_date used: \n",
    "## 1. 2021-12-31, 20211231\n",
    "## 2. 2022-01-14, 20220114\n",
    "start_date, end_date, file_date = \"2020-03-01\", \"2021-12-25\", \"20211225_Lancet_bmi\"\n",
    "who_score_table_version = \"2022_08_30\"\n",
    "## register it so can be directly used in the following SQL statement\n",
    "spark.conf.set(\"startdate.var\", start_date)\n",
    "spark.conf.set(\"enddate.var\", end_date)\n",
    "\n",
    "## Define the instance number\n",
    "intsance_num = 1000\n",
    "spark.conf.set(\"intsance.var\", intsance_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e77400b5-e405-464c-ab4a-67e11490eee1",
     "showTitle": true,
     "title": "Define necessary functions"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[2]: &lt;function __main__.test_results_category(result_list)&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[2]: &lt;function __main__.test_results_category(result_list)&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####################################\n",
    "## Function to get cleaned string\n",
    "## Input: String\n",
    "## Output: String\n",
    "####################################\n",
    "def clean_string(input_string):\n",
    "  \"\"\"Clean an arbitrary input string\n",
    "\n",
    "  Parameters:\n",
    "  input_string (str): An arbitrary input string\n",
    "\n",
    "  Returns:\n",
    "  str: Cleaned string\n",
    "\n",
    "  \"\"\"\n",
    "  ## Return empty string if input_string is None\n",
    "  if input_string is None:\n",
    "    return ''\n",
    "  \n",
    "  ## Characters to remove\n",
    "  remove_char = ['.', '*', ':', '-', '(', ')', '_']\n",
    "  \n",
    "  ## Remove non-alphanumeric characters\n",
    "  cleaned_string = input_string\n",
    "  for char_ in remove_char:\n",
    "    cleaned_string = cleaned_string.replace(char_, '')\n",
    "  \n",
    "  ## Lowercase, remove leading and trailing whitespace, and replace duplicate spaces within the string\n",
    "  cleaned_string = re.sub(' +', ' ', cleaned_string.strip().lower())\n",
    "  \n",
    "  return cleaned_string\n",
    "\n",
    "## Create a UDF\n",
    "clean_string_udf = F.udf(clean_string, StringType())\n",
    "\n",
    "## Register UDFs so they can be used in SQL statements\n",
    "spark.udf.register(\"clean_string\", clean_string)\n",
    "\n",
    "###########################################################################################################################\n",
    "## Function to map result value strings for COVID-19 tests to 'positive', 'negative', 'see report or comment' or 'unknown'\n",
    "## Input: resultvalue column\n",
    "## Output: mapped positiv or negative results\n",
    "## Notice: Please check the positive_strings, negative_strings, report_comment_strings, to include all new edge cases\n",
    "##\n",
    "###########################################################################################################################\n",
    "def covid19_resultvalue_map(resultvalue):\n",
    "  positive_strings = ['positive', 'detected', 'presumptive pos', 'detected see scanned report', 'detcted', 'presumptive positive', 'postive', 'positve','inst positive','presumptive positive see scanned report','added detected', 'detected h', 'detect', 'detected p', 'pos 2019 ncov', 'postiive', 'pos', 'dectected', 'detectd', 'dtected', 'deteccted', 'detectred', 'dected', 'posative', 'ddetected', 'covid 19', 'deteceted', 'detecte', 'detectedd']\n",
    "  \n",
    "  negative_strings = ['none detected', 'undetected', 'not dectected', 'negative', 'not detected', 'not deteced', 'not detected see scanned result', 'not detectd', 'not detected see scanned report', 'negatiev', 'not detected see media', 'non detected', 'not dtected', 'not detected see scanned results', 'not detecte', 'negtive', 'not detected see scanned result', 'presumptive negative', 'negatvie', 'not detecteed',\n",
    "                     'prsmptve neg covid','normal', 'not detect','not detected see', 'neg', 'negitive', 'revised not detected', 'not detected see scaneed report', 'added none detected', 'revised none detected', 'not detectedd', 'not dected', 'not deteccted', 'not deteceted', 'no detected', 'inst negative', 'notdetected', 'not detected test performed at sacred heart', 'noy detected', 'no detected', 'negatiive', \n",
    "                      'negaqtive', 'not reported', 'not detectred', 'neagtive', 'neggative', 'negaive', 'not ddetected', 'ng', 'nf', 'ngeative']\n",
    "  \n",
    "  report_comment_strings = ['see scanned report', 'comment', 'see scanned result', 'abnormal see scanned report', 'see report', 'see scanned results', 'see comments', 'see scanned report covid19', 'see comment', 'please see scanned report', 'see note', 'separate report to follow', 'see attached report', 'refer to separate reference lab report for results', 'other see scanned report', 'refer to separate reference lab report for results', 'See resp pcr']\n",
    "  \n",
    "  ## Clean the resultvalue string\n",
    "  resultvalue_cleaned = clean_string(resultvalue)\n",
    "  \n",
    "  ## Return mapped string\n",
    "  if(resultvalue_cleaned in positive_strings):\n",
    "    return 'positive'\n",
    "  elif(resultvalue_cleaned in negative_strings):\n",
    "    return 'negative'\n",
    "  elif(resultvalue_cleaned in report_comment_strings):\n",
    "    return 'see report or comment'\n",
    "  else:\n",
    "    return 'unknown'\n",
    "\n",
    "## Register UDFs so they can be used in SQL statements\n",
    "spark.udf.register(\"covid19_resultvalue_map\", covid19_resultvalue_map)\n",
    "\n",
    "#################################################################################\n",
    "## Function to assign result value strings for COVID-19 tests into categories\n",
    "## Input: list\n",
    "## Output: list\n",
    "##############################################################################\n",
    "def test_results_category(result_list):\n",
    "  # Categorize patient based on one or more tests\n",
    "  if 'positive' in result_list:\n",
    "    return '>= 1 positive'\n",
    "  elif ('negative' in result_list) and ('positive' not in result_list):\n",
    "    return '>= 1 negative (no positive)'\n",
    "  else:\n",
    "    return 'inconclusive or unknown'\n",
    "\n",
    "## Register UDFs so they can be used in SQL statements\n",
    "spark.udf.register(\"test_results_category\", test_results_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb86344b-c66d-4948-b678-977feb26d9d6",
     "showTitle": true,
     "title": "Get records of all patients tested for COVID-19"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "covid_tests_df = spark.sql(\n",
    "\"\"\"\n",
    "SELECT\n",
    "  e.pat_id,\n",
    "  e.instance,\n",
    "  e.pat_enc_csn_id,\n",
    "  date(e.contact_date),\n",
    "  e.admissiondatetime,\n",
    "  e.encountertype,\n",
    "  e.patientclass,\n",
    "  e.visittype,\n",
    "  po.orderingdatetime,\n",
    "  lr.observationdatetime,\n",
    "  lr.basename,\n",
    "  lr.commonname,  \n",
    "  lr.resultname,\n",
    "  lr.resultvalue,\n",
    "  clean_string(lr.resultvalue) as result_cleaned,\n",
    "  covid19_resultvalue_map(lr.resultvalue) as result_short,\n",
    "  lr.flaggedas\n",
    "FROM rdp_phi.encounter as e\n",
    "JOIN (\n",
    "  -- Join procedure orders for NAAT and PCR tests\n",
    "  SELECT * FROM rdp_phi.procedureorders\n",
    "  WHERE\n",
    "    ordername LIKE '%NAAT%' OR\n",
    "    ordername LIKE '%PCR%') as po\n",
    "ON\n",
    "  e.instance = po.instance AND\n",
    "  e.pat_enc_csn_id = po.pat_enc_csn_id\n",
    "JOIN (\n",
    "  -- Join lab results to get results of COVID-19 tests\n",
    "  SELECT * FROM rdp_phi.labresult\n",
    "  WHERE commonname in ('SARS CORONAVIRUS 2 RNA ORD', 'POC SARS CORONAVIRUS 2 RNA ORD', 'SARS-COV-2 (COVID-19) QUAL PCR RESULT', 'COV19EX','NAA (COVID-19) (REF)')\n",
    "or resultname in ('SARS CORONAVIRUS 2 RNA ORD', 'POC SARS CORONAVIRUS 2 RNA ORD', 'SARS-COV-2 (COVID-19) QUAL PCR RESULT', 'COV19EX','NAA (COVID-19) (REF)') ) as lr\n",
    "ON\n",
    "  po.instance = lr.instance AND\n",
    "  po.order_proc_id = lr.order_proc_id\n",
    "WHERE resultvalue IS NOT NULL\n",
    "and e.CONTACT_DATE >= TIMESTAMP('${startdate.var}')\n",
    "and e.CONTACT_DATE <= TIMESTAMP('${enddate.var}')\n",
    "\"\"\")\n",
    "\n",
    "## Create a table that can be queried using SQL-style syntax\n",
    "covid_tests_df.createOrReplaceTempView('covid_tests')\n",
    "\n",
    "## Print total number of records\n",
    "# print('Total records: %s' % covid_tests_df.count())\n",
    "\n",
    "## Use only for checking on the data\n",
    "## covid_tests_df.filter(\"patientstatus == 'Hospital Inpatient Visit'\").limit(100).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69b8125c-8029-40bf-bfa9-74c42df7db96",
     "showTitle": true,
     "title": "Generate a table with first encounter ID"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_encounter_df = spark.sql(\n",
    "\"\"\"\n",
    "SELECT\n",
    "  pat_id,\n",
    "  instance,\n",
    "  first_pat_enc_csn_id,\n",
    "  encountertype,\n",
    "  patientclass\n",
    "FROM (\n",
    "  SELECT *,\n",
    "    FIRST_VALUE(pat_enc_csn_id) OVER (\n",
    "      PARTITION BY instance, pat_id\n",
    "      ORDER BY contact_date\n",
    "      RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as first_pat_enc_csn_id\n",
    "  FROM covid_tests)\n",
    "GROUP BY instance, pat_id, first_pat_enc_csn_id, encountertype, patientclass\n",
    "ORDER BY pat_id, instance\n",
    "\"\"\")\n",
    "\n",
    "## Create a table that can be queried using SQL-style syntax\n",
    "first_encounter_df.createOrReplaceTempView('first_encounter')\n",
    "\n",
    "\n",
    "## Print total number of records\n",
    "# print('Total records: %s' % first_encounter_df.count())\n",
    "\n",
    "## Use only for checking on the data\n",
    "## first_encounter_df.orderBy('pat_id', 'instance').limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb05aeb0-746e-4d48-8551-71a8af0a22cd",
     "showTitle": true,
     "title": "Group by patient/instance and aggregate test results (revised to include contact date of first positive test)"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "patient_instance_aggregate_df = spark.sql(\n",
    "\"\"\"\n",
    "WITH agg as (\n",
    "SELECT\n",
    "  pat_id,\n",
    "  instance,\n",
    "  concat_ws(\";\", collect_list(pat_enc_csn_id)) as pat_enc_csn_id,\n",
    "  MIN(contact_date) as first_contact_date,\n",
    "  MIN(date_of_positive_test) as covidPositive_first_contact_date,\n",
    "  MIN(date_of_negative_test) as covidNegative_first_contact_date,\n",
    "  DATE(MIN(admissiondatetime)) as first_admit_date,\n",
    "  MAX(contact_date) as last_contact_date,\n",
    "  DATE(MAX(admissiondatetime)) as last_admit_date,\n",
    "  concat_ws(\";\", collect_list(observationdatetime)) as observationdatetime,\n",
    "  concat_ws(\";\", collect_list(result_short)) as result_short,\n",
    "  concat_ws(\";\", collect_list(encountertype)) as encountertype,\n",
    "  concat_ws(\";\", collect_list(patientclass)) as patientclass,\n",
    "  test_results_category(collect_list(result_short)) as results_category,\n",
    "  concat_ws(\";\", collect_list(flaggedas)) as flaggedas,\n",
    "  COUNT(*) as num_tests\n",
    "FROM (\n",
    "  SELECT *,\n",
    "    (CASE WHEN result_short = 'positive' THEN contact_date ELSE NULL END) as date_of_positive_test,\n",
    "    (CASE WHEN result_short = 'negative' THEN contact_date ELSE NULL END) as date_of_negative_test\n",
    "  FROM covid_tests\n",
    "  ORDER BY instance, pat_id, contact_date)\n",
    "GROUP BY instance, pat_id)\n",
    "\n",
    "SELECT\n",
    "  agg.pat_id,\n",
    "  agg.instance,\n",
    "  agg.pat_enc_csn_id,\n",
    "  fe.first_pat_enc_csn_id,\n",
    "  agg.first_contact_date,\n",
    "  agg.covidPositive_first_contact_date,\n",
    "  agg.covidNegative_first_contact_date,\n",
    "  agg.first_admit_date,\n",
    "  agg.last_contact_date,\n",
    "  agg.last_admit_date,\n",
    "  agg.observationdatetime,\n",
    "  agg.result_short,\n",
    "  agg.results_category,\n",
    "  agg.encountertype,\n",
    "  agg.patientclass,\n",
    "  agg.flaggedas,\n",
    "  agg.num_tests\n",
    "FROM agg\n",
    "LEFT JOIN first_encounter as fe\n",
    "ON\n",
    "  agg.instance = fe.instance AND\n",
    "  agg.pat_id = fe.pat_id\n",
    "\"\"\")\n",
    "## Create a table that can be queried using SQL-style syntax\n",
    "patient_instance_aggregate_df.createOrReplaceTempView('patient_instance_aggregate')\n",
    "\n",
    "## Print total number of records\n",
    "# print('Total records: %s' % patient_instance_aggregate_df.count())\n",
    "\n",
    "# patient_instance_aggregate_df.limit(5).toPandas()\n",
    "## Notice: DEFINE PositiveCOVID_contact_dt: The contact date of the encounter(s) when they got their first COVID-19 test positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc24073e-ff1c-4a93-9351-95a2031915df",
     "showTitle": true,
     "title": "Prepare covid_tests_df "
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "covid_tests_df = spark.sql(\n",
    "\"\"\"\\\n",
    "SELECT\n",
    "pat_id, \n",
    "instance, \n",
    "first_contact_date, \n",
    "covidPositive_first_contact_date,\n",
    "covidNegative_first_contact_date,\n",
    "encountertype,\n",
    "patientclass,\n",
    "results_category\n",
    "FROM patient_instance_aggregate\n",
    "\"\"\")\n",
    "\n",
    "## Create a table that can be queried using SQL-style syntax\n",
    "# covid_tests_df.createOrReplaceTempView('covid_tests')\n",
    "\n",
    "covid_tests_df = covid_tests_df.dropDuplicates()\n",
    "\n",
    "## Print total number of records\n",
    "# print('Total records: %s' % covid_tests_df.count())\n",
    "\n",
    "## print the first 20 records\n",
    "# covid_tests_df.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f95b12c-2af4-4876-9ca8-f38f9121331c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Convert results_category as strings with a new column and delete results_category\n",
    "from pyspark.sql.functions import col, when\n",
    "covid_tests_df = covid_tests_df.withColumn(\"results\", when(col(\"results_category\") == \">= 1 negative (no positive)\",\"Negative\")\n",
    "                                 .when(col(\"results_category\") == \">= 1 positive\",\"Positive\")\n",
    "                                 .otherwise(\"Unknown\"))\n",
    "\n",
    "# covid_tests_df.groupBy(\"results\").count().show(truncate=False)\n",
    "# covid_tests_df.groupBy(\"results_category\").count().show(truncate=False)\n",
    "\n",
    "covid_tests_df = covid_tests_df.drop(\"results_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5154df81-79c5-4c35-ab3e-d424a038be77",
     "showTitle": true,
     "title": "Save the checkpoint"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## save the current covid tests results table\n",
    "save_table_name = 'rdp_phi_sandbox.qw_{}_covid_tests'.format(file_date)\n",
    "spark.sql(\"DROP TABLE IF EXISTS rdp_phi_sandbox.qw_{}_covid_tests\".format(file_date))\n",
    "covid_tests_df.write.saveAsTable(save_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35c1d113-bd0e-4d1f-a2d1-58bfa5decf7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Merge covid tests results table and who scores table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a9310df-45aa-490a-8751-78c8a92638c2",
     "showTitle": true,
     "title": "Can resume from this checkpoint"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################################################################\n",
    "# Name of covid test file depends on the previous end date selected\n",
    "################################################################\n",
    "read_table_name = 'rdp_phi_sandbox.qw_{}_covid_tests'.format(file_date)\n",
    "\n",
    "## keep both SARS-CoV-2 Positive tests and negative tests\n",
    "covid_tests_df = spark.sql(\"\"\"SELECT * FROM {}\"\"\".format(read_table_name))\n",
    "\n",
    "## merge pat_id + instance\n",
    "covid_tests_df = covid_tests_df.dropDuplicates([\"pat_id\"]) \\\n",
    "  .withColumn('patient_id', F.concat(F.col('instance'), F.col('pat_id')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cc5959d-5c05-44fd-93c3-aba879c9df30",
     "showTitle": true,
     "title": "Load who scores table"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################################################################################\n",
    "## Notice: Ask Jenn or Yeon Mi to confirm whether should use the 2008 or 2016 version\n",
    "## follow the ceda pipeline to upgrade those tables\n",
    "#######################################################################################\n",
    "pat_who_scores_df = spark.sql(\"\"\"SELECT * FROM rdp_phi_sandbox.hadlock_who_scores_{}\"\"\".format(who_score_table_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2c2a4e7-9c3f-48e0-80e3-d13def03e23c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "covid_wos = pat_who_scores_df.join(covid_tests_df, on = [\"patient_id\"], how = \"inner\").drop(covid_tests_df.patient_id)\n",
    "\n",
    "## save the merged COVID tests and WHO scores table \n",
    "spark.sql(\"DROP TABLE IF EXISTS rdp_phi_sandbox.qw_{}_covidtests_wos2\".format(file_date))\n",
    "covid_wos.write.saveAsTable(\"rdp_phi_sandbox.qw_{}_covidtests_wos2\".format(file_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c751524-6ec2-48b1-90ab-455b269eec31",
     "showTitle": true,
     "title": "Keep the pat_id and covidPositive_first_contact_date in a separate data frame"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "covidPositive_first_contact_date_df = covid_wos.select(\"patient_id\", \"covidPositive_first_contact_date\", \"covidNegative_first_contact_date\").dropDuplicates()\n",
    "# covidPositive_first_contact_date_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a8eecd-97cf-48f0-8fd5-4c5ab79d858a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################\n",
    "## Define utility for getting max or min date from array\n",
    "## Input: list\n",
    "########################################################\n",
    "def get_list_value(l, val='max'):\n",
    "  if ((l is None) or not(isinstance(l, list))):\n",
    "    return None\n",
    "  if (len(l) < 1):\n",
    "    return None\n",
    "  if (val == 'max'):\n",
    "    return max(l)\n",
    "  elif (val == 'min'):\n",
    "    return min(l)\n",
    "  else:\n",
    "    return None\n",
    "min_date_udf = F.udf(lambda x: get_list_value(x, val='min'), TimestampType())\n",
    "max_date_udf = F.udf(lambda x: get_list_value(x, val='max'), TimestampType())\n",
    "\n",
    "## Load the merged covid test results and WHO table\n",
    "table_name = \"rdp_phi_sandbox.qw_{}_covidtests_wos2\".format(file_date)\n",
    "select_cols = ['patient_id', 'record_dt', 'who_score', 'first_contact_date', 'encountertype', 'patientclass', 'results']\n",
    "who_data_df = spark.sql(\"SELECT * FROM {}\".format(table_name)) \\\n",
    "  .where(F.col('record_dt') >= F.date_sub('first_contact_date', 2)) \\\n",
    "  .select(*select_cols, min_date_udf(F.col('admission_dt')).alias('admission_dt'),\n",
    "          min_date_udf(F.col('discharge_dt')).alias('discharge_dt')) \\\n",
    ".withColumn('encounter_duration',\n",
    "            F.round((F.unix_timestamp('discharge_dt') - F.unix_timestamp('admission_dt'))/86400, 2)) \\\n",
    ".dropDuplicates()\n",
    "\n",
    "## Get first encounter after contact date for each patient\n",
    "partition_by_cols = ['patient_id', 'first_contact_date']\n",
    "w = Window.partitionBy(*partition_by_cols).orderBy('record_dt') \\\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "first_encounter_df = who_data_df \\\n",
    "  .select(*partition_by_cols, F.first('admission_dt').over(w).alias('first_admission_dt'),\n",
    "          F.first('discharge_dt').over(w).alias('first_discharge_dt')) \\\n",
    "  .where(F.col('admission_dt') == F.col('first_admission_dt')).dropDuplicates()\n",
    "\n",
    "## Get first, max and last WHO score for each patient\n",
    "partition_by_cols = ['patient_id', 'first_contact_date', 'admission_dt', 'discharge_dt']\n",
    "w = Window.partitionBy(*partition_by_cols).orderBy('record_dt') \\\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "patient_who_data_df = first_encounter_df \\\n",
    "  .withColumnRenamed('first_admission_dt', 'admission_dt') \\\n",
    "  .withColumnRenamed('first_discharge_dt', 'discharge_dt') \\\n",
    "  .join(who_data_df, partition_by_cols, how='left') \\\n",
    "  .select(*partition_by_cols, 'encountertype', 'patientclass', 'results', 'encounter_duration',\n",
    "          F.first('who_score').over(w).alias('who_score_first'),\n",
    "          F.max('who_score').over(w).alias('who_score_max'),\n",
    "          F.last('who_score').over(w).alias('who_score_last')) \\\n",
    "  .dropDuplicates()\n",
    "\n",
    "## Get datetime where max WHO score was first reached\n",
    "join_cols = ['patient_id', 'who_score', 'admission_dt', 'discharge_dt']\n",
    "w = Window.partitionBy(join_cols).orderBy('record_dt') \\\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "max_who_scores_df = patient_who_data_df \\\n",
    "  .select('patient_id', 'who_score_max', 'admission_dt', 'discharge_dt') \\\n",
    "  .withColumnRenamed('who_score_max', 'who_score') \\\n",
    "  .join(who_data_df.select(*join_cols, 'record_dt'), join_cols, how='inner') \\\n",
    "  .select(*join_cols, F.first('record_dt').over(w).alias('max_who_score_record_dt')) \\\n",
    "  .dropDuplicates()\n",
    "\n",
    "## Merge to get final dataframe\n",
    "join_cols = ['patient_id', 'admission_dt', 'discharge_dt']\n",
    "who_score_summary_df = patient_who_data_df \\\n",
    "  .join(max_who_scores_df.select(*join_cols, 'max_who_score_record_dt'), join_cols, how='left') \\\n",
    "  .withColumn('days_to_max_who_score',\n",
    "              F.round((F.unix_timestamp('max_who_score_record_dt') - F.unix_timestamp('admission_dt'))/86400, 2)) \\\n",
    "  .withColumn('days_hospitalized', F.col('encounter_duration')) \\\n",
    "  .withColumn('days_to_min_who_score', F.col('days_hospitalized') - F.col('days_to_max_who_score')) \\\n",
    "  .where(F.col('admission_dt') >= start_date )\n",
    "\n",
    "## Merge back the covidPositive_first_contact_date column\n",
    "who_score_summary_df = who_score_summary_df.join(covidPositive_first_contact_date_df, on = \"patient_id\", how = \"left\").drop(covidPositive_first_contact_date_df.patient_id)\n",
    "\n",
    "# # Save the table\n",
    "# spark.sql(\"DROP TABLE IF EXISTS rdp_phi_sandbox.qw_{}_covid_wos_tests\".format(file_date))\n",
    "# who_score_summary_df.write.saveAsTable(\"rdp_phi_sandbox.qw_{}_covid_wos_tests\".format(file_date))\n",
    "\n",
    "# ## refresh the table\n",
    "# spark.sql(\"REFRESH TABLE rdp_phi_sandbox.qw_{}_covid_wos_tests\".format(file_date))\n",
    "\n",
    "# ## Print out the first 3 entries\n",
    "# who_score_summary_df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf21e5b7-2e3c-4076-ba9c-1dbf92405fbb",
     "showTitle": true,
     "title": "Save the temp table as results"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[14]: DataFrame[]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[14]: DataFrame[]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Save the table\n",
    "spark.sql(\"DROP TABLE IF EXISTS rdp_phi_sandbox.qw_{}_covid_wos_tests\".format(file_date))\n",
    "who_score_summary_df.write.saveAsTable(\"rdp_phi_sandbox.qw_{}_covid_wos_tests\".format(file_date))\n",
    "\n",
    "## refresh the table\n",
    "spark.sql(\"REFRESH TABLE rdp_phi_sandbox.qw_{}_covid_wos_tests\".format(file_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46494627-108d-44db-a7ac-323c9374fcbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# who_score_summary_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b502d59-ca38-4e72-ba84-d8ca2c96b9fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## New definition of hospitalization\n",
    "* patient status is Admission or discharged\n",
    "* patient class is Inpatient (combined with knowledge in the adtevent table)\n",
    "* COVID-test is positive\n",
    "* First positive observation date - 3 days <= hospitalization admission date (make sure hospitalization is an outcome of covid)\n",
    "* hospitalization admission date (make sure hospitalization is an outcome of covid) <= First positive observation date + 14 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ef4fb56-3fb4-4be1-82a9-8eb26b17e5dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_df = spark.sql(\n",
    "\"\"\"\n",
    "SELECT\n",
    "  e.pat_id,\n",
    "  e.instance,\n",
    "  e.pat_enc_csn_id,\n",
    "  date(e.contact_date),\n",
    "  e.admissiondatetime,\n",
    "  e.encountertype,\n",
    "  e.patientclass,\n",
    "  e.patientstatus,\n",
    "  lr.observationdatetime,\n",
    "  lr.resultvalue,\n",
    "  clean_string(lr.resultvalue) as result_cleaned,\n",
    "  covid19_resultvalue_map(lr.resultvalue) as result_short,\n",
    "  lr.flaggedas\n",
    "FROM rdp_phi.encounter as e\n",
    "JOIN (\n",
    "  -- Join procedure orders for NAAT and PCR tests\n",
    "  SELECT * FROM rdp_phi.procedureorders\n",
    "  WHERE\n",
    "    ordername LIKE '%NAAT%' OR\n",
    "    ordername LIKE '%PCR%') as po\n",
    "ON\n",
    "  e.instance = po.instance AND\n",
    "  e.pat_enc_csn_id = po.pat_enc_csn_id\n",
    "JOIN (\n",
    "  -- Join lab results to get results of COVID-19 tests\n",
    "  SELECT * FROM rdp_phi.labresult\n",
    "  WHERE commonname in ('SARS CORONAVIRUS 2 RNA ORD', 'POC SARS CORONAVIRUS 2 RNA ORD', 'SARS-COV-2 (COVID-19) QUAL PCR RESULT', 'COV19EX','NAA (COVID-19) (REF)')\n",
    "or resultname in ('SARS CORONAVIRUS 2 RNA ORD', 'POC SARS CORONAVIRUS 2 RNA ORD', 'SARS-COV-2 (COVID-19) QUAL PCR RESULT', 'COV19EX','NAA (COVID-19) (REF)') ) as lr\n",
    "ON\n",
    "  po.instance = lr.instance AND\n",
    "  po.order_proc_id = lr.order_proc_id\n",
    "WHERE resultvalue IS NOT NULL\n",
    "and e.CONTACT_DATE >= TIMESTAMP('${startdate.var}')\n",
    "and e.CONTACT_DATE <= TIMESTAMP('${enddate.var}')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bc3369a-2eb1-43e9-9a79-1c30b43dbbb0",
     "showTitle": true,
     "title": "Add additional inpatient info from the adtevent table"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Get necessary columns from the adtevent table\n",
    "adtevent_full_df = spark.sql(\"\"\"select distinct PAT_ID, PAT_ENC_CSN_ID, INSTANCE, BASEPATIENTCLASS, PATIENTCLASS, EVENTTYPE, FIRSTINPATIENT, EVENTTIMESTAMP from rdp_phi.adtevent\"\"\")\n",
    "\n",
    "\n",
    "## Based on checking actual table, there are many nulls can be fill using the Patientclass column (inpatient or outpatient)\n",
    "from pyspark.sql import functions as F\n",
    " \n",
    "## Create a new column with only inpatient and outpatient info\n",
    "adtevent_new_df = adtevent_full_df.withColumn(\"patientclass_only_inpatients_outpatients\", F.when(F.col(\"PATIENTCLASS\").isin(\"Inpatient\", \"Outpatient\"), F.col(\"PATIENTCLASS\") )\\\n",
    "                                             .otherwise(F.lit(None)) )\n",
    " \n",
    "## Use the new column to fill as many null as possible in the BASEPATIENTCLASS column\n",
    "adtevent_new_df = adtevent_new_df.withColumn(\"New_patientclass\", F.when( F.col(\"BASEPATIENTCLASS\").isNull(),  F.col(\"patientclass_only_inpatients_outpatients\") )\\\n",
    "                                            .otherwise(F.col(\"BASEPATIENTCLASS\") ) )\n",
    "\n",
    "## Use the logic of eventtype (admission or transfer in) and firstinpatient (yes) to fill as many as nulls again\n",
    "## Create a new column with only eventtype.isin(\"Admission\", \"Transfer In\") AND firstinpatient.isin(\"Y\") are set to \"Inpatient\"\n",
    "adtevent_new2_df = adtevent_new_df.withColumn(\"Admission_or_TransferIn_and_firstinpatientYes\", F.when( (F.col(\"eventtype\").isin(\"Admission\", \"Transfer In\") ) & ( F.col(\"firstinpatient\").isin(\"Y\") ), \"Inpatient\" )\\\n",
    "                                             .otherwise(F.lit(None)) )\n",
    "\n",
    "## Use the new column to fill as many null as possible in the BASEPATIENTCLASS column\n",
    "adtevent_new2_df = adtevent_new2_df.withColumn(\"New_patientclass\", F.when( F.col(\"New_patientclass\").isNull(),  F.col(\"Admission_or_TransferIn_and_firstinpatientYes\") )\\\n",
    "                                            .otherwise(F.col(\"New_patientclass\") ) )\n",
    "\n",
    "\n",
    "#####################################################\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "######################################################\n",
    "\n",
    "## Only filter those patients are more confident to be inpatient \n",
    "## relax the selection to no need of limit to firstinpatient == Y\n",
    "adtevent_inpatient_df = adtevent_new2_df.where( F.col(\"New_patientclass\") == \"Inpatient\" ).dropDuplicates()\n",
    "\n",
    "## Drop those \"helping\" columns\n",
    "cols_to_drop = (\"patientclass_only_inpatients_outpatients\", \"Admission_or_TransferIn_and_firstinpatientYes\", \"BASEPATIENTCLASS\", \"PATIENTCLASS\",\n",
    "                \"PAT_ID\", \"INSTANCE\")\n",
    "adtevent_inpatient_df = adtevent_inpatient_df.drop(*cols_to_drop).dropDuplicates()\n",
    "\n",
    "## Create a window to only take the first row of event time stamp as the first hospitalization date\n",
    "w = Window.partitionBy(\"PAT_ENC_CSN_ID\").orderBy(col(\"EVENTTIMESTAMP\").asc())\n",
    "\n",
    "adtevent_inpatient_Y_df = adtevent_inpatient_df.withColumn(\"row\",row_number().over(w)) \\\n",
    "  .filter(col(\"row\") == 1).drop(\"row\")\n",
    "\n",
    "## Drop those additional columns not needed for later left join\n",
    "cols_to_drop = (\"EVENTTYPE\")\n",
    "adtevent_new3_df = adtevent_inpatient_Y_df.drop(*cols_to_drop).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48800dbe-2534-477e-95a3-82764d516cd8",
     "showTitle": true,
     "title": "Left join those two data frames to get a combined inpatients info"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Left join with the pat_enc_csn_id column\n",
    "# hos_df = temp_df.join(adtevent_new3_df, (temp_df.pat_enc_csn_id == adtevent_new3_df.PAT_ENC_CSN_ID) & (temp_df.admissiondatetime == adtevent_new3_df.EVENTTIMESTAMP), how = \"left\").drop(adtevent_new3_df.PAT_ENC_CSN_ID)\n",
    "\n",
    "## Checked, the row counts == distinct pat_enc code which means each id now only have one time record.\n",
    "hos_df = temp_df.join(adtevent_new3_df, (temp_df.pat_enc_csn_id == adtevent_new3_df.PAT_ENC_CSN_ID), how = \"left\").drop(adtevent_new3_df.PAT_ENC_CSN_ID)\n",
    "\n",
    "# temp2_df.limit(50).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93a2aa91-abfe-47d0-a005-23d19a71ff40",
     "showTitle": true,
     "title": "Combine info in both patient class and new patient class columns"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "patient_status_list = ['Admission', 'Discharged']\n",
    "\n",
    "hos2_df = hos_df.withColumn(\"hos_patient_class\", F.when( ( (F.col(\"patientclass\") == \"Inpatient\")&(F.col(\"patientstatus\").isin(patient_status_list)) )|(F.col(\"new_patientclass\") == \"Inpatient\"), \"Inpatient\")\\\n",
    "                           .otherwise(F.lit(None)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "997e9979-e239-4ef4-9f6f-48f75e12c602",
     "showTitle": true,
     "title": "Choose between event time stamp and admission time by adding back info in the admission time col"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hos2_df = hos2_df.withColumn(\"hos_earliest_time\", F.when( (F.col(\"hos_patient_class\") == \"Inpatient\") & ( (F.col(\"admissiondatetime\").isNull() )|(F.col(\"admissiondatetime\") > F.col(\"EVENTTIMESTAMP\")) ), F.col(\"EVENTTIMESTAMP\") )\\\n",
    "                                             .when((F.col(\"hos_patient_class\") == \"Inpatient\"),  F.col(\"admissiondatetime\"))\\\n",
    "                                             .otherwise(F.lit(None)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f43e61f5-7eb1-463f-9aa0-bcf7276247c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display(hos2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cd1fbfd-ba0e-4c59-bdfe-9253ef985a26",
     "showTitle": true,
     "title": "Define Hospitalization_after_positive outcome based on patient status and patient class (only positive patients)"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Overwrite the previous hos dataframe\n",
    "hos_df = hos2_df.select(\"*\")\n",
    "\n",
    "## Change to use the new patientclass column instead of the old one\n",
    "## no need to limit to only firstinpatient == \"Y\"\n",
    "hos_df = hos_df.withColumn(\"hospitalized_pos\",\\\n",
    "                                              when((hos_df.result_short == 'positive')\\\n",
    "                                                    # ( (hos_df.patientstatus.isin(patient_status_list)) | (hos_df.FIRSTINPATIENT == \"Y\") )\\\n",
    "                                                    & (hos_df.hos_patient_class == 'Inpatient'),'yes')\\\n",
    "                                               .otherwise('no'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5c6e0b2-1037-42eb-9929-c584f7da89bd",
     "showTitle": true,
     "title": "Change positive patient who were already hospitalized at the time of first positive COVID test to be \"no\" for hospitalized_after_positive"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, first\n",
    "\n",
    "###Find the minimum positive test time\n",
    "hos_df = hos_df.withColumn(\"pos_observationdatetime\",\\\n",
    "                       when(hos_df.result_short == 'positive', hos_df.observationdatetime)\\\n",
    "                       .otherwise(F.lit(None)))\n",
    "\n",
    "## Notice the reason to use first() instead of min() is because the min function of spark will not ignore NULL value, and first will with ignorenulls = True\n",
    "## In SQL it won't have a question cause, the MIN, MAX in sql will always exclude NULL\n",
    "w = Window.partitionBy(['pat_id','instance']).orderBy(\"pos_observationdatetime\").rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "hos_df = hos_df.withColumn(\"min_positive_obs\", first('pos_observationdatetime', ignorenulls=True).over(w))\n",
    "\n",
    "## Our definition of hospitalization is that admission date is within [min_positive - 3 , min_positive + 14]\n",
    "## Date addition use function: date_add\n",
    "## Date subtraction use function: date_sub\n",
    "\n",
    "## Check if the earliest hospitalization date is within window or not\n",
    "hos_df = hos_df.withColumn(\"hos_before\", when((hos_df.hospitalized_pos == \"yes\") &\\\n",
    "                                              (F.date_sub(hos_df.min_positive_obs, 3) <= hos_df.hos_earliest_time) &\\\n",
    "                                              (hos_df.hos_earliest_time <= F.date_add(hos_df.min_positive_obs, 14)),\"In\")\\\n",
    "                       .otherwise(\"Out\"))\n",
    "\n",
    "\n",
    "hos_df = hos_df.withColumn(\"hospitalized_after_positive\", when((hos_df.hospitalized_pos == \"yes\") & (hos_df.hos_before == \"In\"), F.lit(1))\\\n",
    "                       .otherwise(F.lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d1b7299-a0bc-40f3-9ebd-ad01f2d2e658",
     "showTitle": true,
     "title": "Keep the pat_id and hospitalized_after_positive in a separate data frame"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## merge pat_id + instance\n",
    "hos_df = hos_df.withColumn('patient_id', F.concat(F.col('instance'), F.col('pat_id')))\n",
    "\n",
    "keep_cols = [\"patient_id\", \"hospitalized_after_positive\"]\n",
    "hospitalization_pat_id_df = hos_df.select(*keep_cols).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12df9809-7142-40cd-83a3-4a2c5bafbab0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hospitalization_pat_id_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d7399dd-05ab-4c40-83e9-ca773767cb1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hospitalization_pat_id_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dea89591-1f9e-4278-92f9-71b026965f0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hospitalization_pat_id_df.select('patient_id', 'hospitalized_after_positive').dropDuplicates().groupBy('hospitalized_after_positive').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6e92d3d-fad3-488d-9d8c-0c7bc4b40598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# who_score_summary_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e596f25-ea1d-40a1-bf77-b12530b62b65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Merge back the hospitalized_after_positive column\n",
    "# who_score_summary_df = who_score_summary_df.join(hospitalization_pat_id_df, on = \"patient_id\", how = \"left\").drop(hospitalization_pat_id_df.patient_id)\n",
    "\n",
    "who_score_summary_df_new = who_score_summary_df.join(hospitalization_pat_id_df, on = \"patient_id\", how = \"inner\").drop(hospitalization_pat_id_df.patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c1e87f-f707-42b1-8385-b4b97288a215",
     "showTitle": true,
     "title": "Checkpoint, save the table"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[30]: DataFrame[]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[30]: DataFrame[]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Save the table\n",
    "spark.sql(\"DROP TABLE IF EXISTS rdp_phi_sandbox.qw_{}_covid_wos_tests_hos\".format(file_date))\n",
    "who_score_summary_df_new.write.saveAsTable(\"rdp_phi_sandbox.qw_{}_covid_wos_tests_hos\".format(file_date))\n",
    "\n",
    "## refresh the table\n",
    "spark.sql(\"REFRESH TABLE rdp_phi_sandbox.qw_{}_covid_wos_tests_hos\".format(file_date))\n",
    "\n",
    "## Print out the first 3 entries\n",
    "# who_score_summary_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efbfbe77-1493-4a67-a023-f073edeb2a83",
     "showTitle": true,
     "title": "Count number of covid positive and negative results in the merged table"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = \"qw_{}_covid_wos_tests_hos\".format(file_date)\n",
    "covid_wos_tests_df = spark.sql(\"SELECT * FROM rdp_phi_sandbox.{}\".format(table_name))\n",
    "\n",
    "## maybe need cleanup?\n",
    "# covid_wos_tests_df.groupBy(\"results\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cac34bd-d45d-4c30-ac76-24ecab5db4fb",
     "showTitle": true,
     "title": "Print out the merged COVID test results + WHO score table"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring, length, col, expr\n",
    "covid_wos_tests_df = covid_wos_tests_df.withColumn(\"pat_id\", expr(\"substring(patient_id, 5, 20)\"))\n",
    "## sanity check\n",
    "# covid_wos_tests_df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4a49cb7-9ca9-4ceb-9cc5-8d408e8a341e",
     "showTitle": true,
     "title": "Create the decided_index_date based on first positive contact date, or if null then first negative contact date"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "covid_wos_tests_df = covid_wos_tests_df.withColumn(\"decided_index_date\", when(covid_wos_tests_df.covidPositive_first_contact_date.isNull(), covid_wos_tests_df.covidNegative_first_contact_date)\n",
    "                                                   .otherwise(covid_wos_tests_df.covidPositive_first_contact_date)\n",
    "  )\n",
    "## sanity check\n",
    "# covid_wos_tests_df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ff2bd58-3e05-402b-bb39-a1cae6eaf5f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### COVID19 + WHO Scores Task completed :)\n",
    "-> Next: get patient and patient race table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f8a52ce-eeb2-495e-bb00-768d7e0870a3",
     "showTitle": true,
     "title": "Use external notebook to generate the patient & race data frame (Note: check the file path)"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[36]: DataFrame[pat_id: string, patientmrn: string, instance: decimal(4,0), sex: string, birthdate: timestamp, ethnicgroup: string, date_of_death: date, race: string]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[36]: DataFrame[pat_id: string, patientmrn: string, instance: decimal(4,0), sex: string, birthdate: timestamp, ethnicgroup: string, date_of_death: date, race: string]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run\n",
    "\"/Users/qi.wei1@providence.org/working_folder/IMIDs-COVID19-projects/related_libraries/qw_mainEHR(Patient_Patientrace)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a45a5c8-2424-4582-a150-a9e4e4f6bc78",
     "showTitle": true,
     "title": "Separate age groups for covid-19 patients"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Join pos_outcomes and patient tables\n",
    "df1 = pats.join(covid_wos_tests_df, on=[\"pat_id\"], how = 'left').drop(pats.instance)\n",
    "\n",
    "## remove duplicates that were added after joining patient tables\n",
    "df1 = df1.drop_duplicates(['pat_id'])\n",
    "\n",
    "## Derived columns: 'age' in years from birthdate\n",
    "## Using the \"decided_index_date\" to decide the patients' age at first positive or negative date\n",
    "from pyspark.sql.functions import datediff, to_date, lit\n",
    "covid_pats  = df1.withColumn(\"age\", F.bround(datediff(col(\"decided_index_date\"), col(\"birthdate\"))/365))\n",
    "covid_pats = covid_pats.filter(col(\"results\") != \"Unknown\")\n",
    "\n",
    "## age_ranges from age\n",
    "from pyspark.sql.functions import udf\n",
    "age_range = udf(lambda age:'0-17' if(age >=0 and age < 18) else\n",
    "                           '18-49'if (age >= 18 and age < 50) else\n",
    "                           '50-74' if (age >= 50 and age < 75) else\n",
    "                           '75+' if (age >= 75) else '')\n",
    "covid_pats  = covid_pats.withColumn('age_range', age_range(covid_pats.age))\n",
    "# drop_cols = ['birthdate', 'date_of_death']\n",
    "drop_cols = ['birthdate']\n",
    "covid_pats = covid_pats.drop(*drop_cols)\n",
    "# print('covid_pats with all ages: {}'.format(covid_pats.count()))\n",
    "\n",
    "######################################################\n",
    "## After comment we decided to use all age patients\n",
    "######################################################\n",
    "## get only adult patients\n",
    "# covid_adult_pats = covid_pats.filter(col(\"age\") >= \"18\")\n",
    "# print('covid_pats >=18 years: {}'.format(covid_adult_pats.count()))\n",
    "# covid_adult_pats.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00f05904-f3af-4fb2-ba0b-4d6e52df347b",
     "showTitle": true,
     "title": "Save the checkpoint"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Save the adults table\n",
    "spark.sql(\"DROP TABLE IF EXISTS rdp_phi_sandbox.qw_{}_all_age_patients_covid_wos_tests\".format(file_date))\n",
    "covid_pats.write.saveAsTable(\"rdp_phi_sandbox.qw_{}_all_age_patients_covid_wos_tests\".format(file_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ef0bf9-a11f-482f-97ac-407ad83dd674",
     "showTitle": true,
     "title": "Count covid-19 positive and negative patients number"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# covid_pats.groupBy(col(\"results\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a83fabf-65c2-4480-bcba-c514728024ed",
     "showTitle": true,
     "title": "Count covid-19 positive and negative patients' max who scores distribution"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# covid_pats.groupBy(col(\"results\"), col(\"who_score_max\")).count().orderBy(\"who_score_max\", \"results\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b45570b-c4ed-4927-b5ad-c482eeec39a2",
     "showTitle": true,
     "title": "Can resume from this checkpoint"
    }
   },
   "source": [
    "### Cleanup covid_adult_tests in R\n",
    "- create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "207bbec4-2129-463f-bf7a-0c4478860f54",
     "showTitle": true,
     "title": "Drop 'encountertype', 'patientclass','max_who_score_record_dt' cols"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[41]: </div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[41]: </div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>results</th>\n",
       "      <th>count(pat_id)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>187934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>1458900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>results</th>\n      <th>count(pat_id)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Positive</td>\n      <td>187934</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Negative</td>\n      <td>1458900</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "covid_pats = spark.sql(\"\"\"SELECT * FROM rdp_phi_sandbox.qw_{}_all_age_patients_covid_wos_tests\"\"\".format(file_date))\n",
    "# cols = ['encountertype', 'patientclass','max_who_score_record_dt']\n",
    "cols = ['encountertype', 'patientclass']\n",
    "covid_pats = covid_pats.drop(*cols)\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "## Print out the number of patient counts for each results\n",
    "covid_pats.groupBy('results').agg(countDistinct('pat_id')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b24ba05d-b788-4807-9d77-b0cc36add7a2",
     "showTitle": true,
     "title": "Save the table"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS rdp_phi_sandbox.qw_{}_all_age_covid_patients2\".format(file_date))\n",
    "covid_pats.write.saveAsTable(\"rdp_phi_sandbox.qw_{}_all_age_covid_patients2\".format(file_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af6930d5-8f62-40ce-9000-9a4da14aef9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# covid_pats.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60e9b2c8-0b41-4e21-bfc7-43bfb775524a",
     "showTitle": true,
     "title": "Checking mixed races (there is now a 1:1 pat_id and race match, no mixed races exist)"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pyspark.sql.functions as func\n",
    "# covid_pats.groupBy('pat_id').agg(func.countDistinct('race')).orderBy('count(race)', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf40c11-572d-4e35-b66f-012e5151468d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create a clean adult patients covid tests and use this for downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7514ea31-5a29-4399-9e60-bc6ec972fdb8",
     "showTitle": true,
     "title": "Save the checkpoint"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################\n",
    "## Pre-processing\n",
    "######################\n",
    "## Read dataframe\n",
    "covid_pats = spark.sql(\"\"\"SELECT * FROM rdp_phi_sandbox.qw_{}_all_age_covid_patients2\"\"\".format(file_date))\n",
    "\n",
    "##########################################################################\n",
    "## Instead of omitting Unknown, Other and None values of sex\n",
    "## We group them into a new Unkonw class together, which means missing\n",
    "#########################################################################\n",
    "#Replace part of string with another string\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# covid_pats = covid_pats.filter(col(\"sex\") != \"Unknown\").filter(col(\"sex\") != \"Other\").filter(col(\"sex\") != \"None\")\n",
    "covid_pats = covid_pats.withColumn(\"sex\", regexp_replace(\"sex\", \"Other\", \"Unknown\"))\n",
    "## Fill all None values with Unknown\n",
    "covid_pats = covid_pats.fillna(value=\"Unknown\",subset=[\"sex\"])\n",
    "\n",
    "## Ethnicity was mapped using both ethnicgroup and race columns\n",
    "## source: https://www.census.gov/newsroom/blogs/random-samplings/2021/08/measuring-racial-ethnic-diversity-2020-census.html\n",
    "from pyspark.sql.functions import col, when, to_date\n",
    "\n",
    "## normalize ethnicity\n",
    "## Notice: Here is the place to separate Unknown and Other race definitions\n",
    "## Fill all None values with Unknown\n",
    "covid_pats = covid_pats.fillna(value=\"Unknown\",subset=[\"race\"])\n",
    "covid_pats = covid_pats.fillna(value=\"Unknown\",subset=[\"ethnicgroup\"])\n",
    "\n",
    "covid_pats = covid_pats.withColumn(\"ethnicity\", \n",
    "                                     when((col(\"ethnicgroup\") == \"Hispanic or Latino\") & (col(\"race\") == \"Other\"), \"Hispanic\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"Hispanic or Latino\") & (col(\"race\") == \"Patient Refused\"), \"Hispanic\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"Hispanic or Latino\") & (col(\"race\") == \"Unknown\"), \"Hispanic\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"Hispanic or Latino\") & (col(\"race\") == \"White or Caucasian\"), \"Hispanic\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"None\") & (col(\"race\") == \"Other\"), \"Unknown\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"Patient Refused\") & (col(\"race\") == \"Other\"), \"Unknown\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"Unknown\") & (col(\"race\") == \"Other\"), \"Unknown\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"None\") & (col(\"race\") == \"Patient Refused\"), \"Unknown\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"Patient Refused\") & (col(\"race\") == \"Patient Refused\"), \"Unknown\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"Unknown\") & (col(\"race\") == \"Patient Refused\"), \"Unknown\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"None\") & (col(\"race\") == \"Unknown\"), \"Unknown\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"Patient Refused\") & (col(\"race\") == \"Unknown\"), \"Unknown\")\\\n",
    "                                     .when((col(\"ethnicgroup\") == \"Unknown\") & (col(\"race\") == \"Unknown\"), \"Unknown\").otherwise(\"Not Hispanic\")).drop(covid_pats.ethnicgroup)\n",
    "## normalize race\n",
    "## figure out how to do mixed races\n",
    "covid_pats = covid_pats.withColumn(\"race1\",\n",
    "                                     when((col(\"race\") == \"American Indian or Alaska Native\"), \"AIAN\")\\\n",
    "                                     .when((col(\"race\") == \"Asian\"), \"Asian\")\\\n",
    "                                     .when((col(\"race\") == \"Black or African American\"), \"Black\")\\\n",
    "                                     .when((col(\"race\") == \"Native Hawaiian or Other Pacific Islander\"), \"NHPI\")\\\n",
    "                                     .when((col(\"race\") == \"Other\"), \"Other\")\\\n",
    "                                     ## Separate the unknown as an independant race factor\n",
    "                                     .when((col(\"race\") == \"Patient Refused\"), \"Unknown\")\\\n",
    "                                     .when((col(\"race\") == \"Unknown\"), \"Unknown\")\\\n",
    "                                     .when((col(\"race\") == \"White or Caucasian\"), \"White\")).drop(covid_pats.race)\n",
    "\n",
    "## normalize race Other using ethnicity                     \n",
    "covid_pats = covid_pats.withColumn(\"race_v2\",\n",
    "                                     when((col(\"race1\") == \"Other\") & (col(\"ethnicity\") == \"Hispanic\"), \"Other Hispanic\")\\\n",
    "                                     .when((col(\"race1\") == \"Other\") & (col(\"ethnicity\") == \"Not Hispanic\"), \"Other Not Hispanic\")\\\n",
    "                                     .when((col(\"race1\") == \"Other\") & (col(\"ethnicity\") == \"Hispanic\"), \"Unknown Hispanic\")\\\n",
    "                                     .when((col(\"race1\") == \"Other\") & (col(\"ethnicity\") == \"Not Hispanic\"), \"Unknown Not Hispanic\").otherwise(col(\"race1\")))\n",
    "                                     \n",
    "## combine ethnicity and race\n",
    "covid_pats = covid_pats.withColumn(\"ethnicity_race\",\n",
    "                                     when((col(\"race_v2\") == \"White\") & (col(\"ethnicity\") == \"Hispanic\"), \"Hispanic White\")\\\n",
    "                                     .when((col(\"race_v2\") == \"White\") & (col(\"ethnicity\") == \"Not Hispanic\"), \"Not Hispanic White\")\\\n",
    "                                     .when((col(\"race_v2\") == \"Black\") & (col(\"ethnicity\") == \"Not Hispanic\"), \"Not Hispanic Black\")\\\n",
    "                                     .when((col(\"race_v2\") == \"AIAN\") & (col(\"ethnicity\") == \"Not Hispanic\"), \"Not Hispanic AIAN\")\\\n",
    "                                     .when((col(\"race_v2\") == \"Asian\") & (col(\"ethnicity\") == \"Not Hispanic\"), \"Not Hispanic Asian\")\\\n",
    "                                     .when((col(\"race_v2\") == \"NHPI\") & (col(\"ethnicity\") == \"Not Hispanic\"), \"Not Hispanic NHPI\")\\\n",
    "                                     .when((col(\"race_v2\") == \"Other\") & (col(\"ethnicity\") == \"Unknown\"), \"Unknown Other\").otherwise(col(\"race_v2\")))\n",
    "\n",
    "## Patient outcomes\n",
    "## hospitalized\n",
    "## Old one for posters\n",
    "# adult_covid = adult_covid.withColumn(\"hospitalized\",\n",
    "#                                      when((col(\"who_score_max\") <= 3), \"no\")\\\n",
    "#                                      .when(col(\"days_hospitalized\").isNull(), \"no\")\\\n",
    "#                                      .otherwise(\"yes\"))\n",
    "\n",
    "## Sevda's hospitalized definition\n",
    "## Now use the \"hospitalized_after_positive\" column\n",
    "\n",
    "###################################################\n",
    "## Invasive mechanical ventilation (IMV)\n",
    "## Now use the \"IMV_after_positive\" column\n",
    "###################################################\n",
    "## Old, need cleanup\n",
    "# covid_pats = covid_pats.withColumn(\"invasive_mechanical_vent\",\\\n",
    "#                                      when((col(\"who_score_max\") == 6) | (col(\"who_score_max\") ==7), \"yes\")\\\n",
    "#                                      .when(col(\"who_score_max\").isNull(), \"no\")\\\n",
    "#                                      .otherwise(\"no\"))\n",
    "\n",
    "covid_pats = covid_pats.withColumn(\"max_who_date\",to_date(\"max_who_score_record_dt\"))\n",
    "covid_pats = covid_pats.withColumn(\"IMV_after_positive\",\\\n",
    "                                     when( ( (col(\"results\") == \"Positive\") & ((col(\"who_score_max\") == 6) | (col(\"who_score_max\") ==7)) &\\\n",
    "                                            (col(\"max_who_date\")>=col(\"covidPositive_first_contact_date\")) & (col(\"max_who_date\")<= F.date_add(col(\"covidPositive_first_contact_date\"), 30) )\\\n",
    "                                           ), F.lit(1))\\\n",
    "                                     .when(col(\"who_score_max\").isNull(), F.lit(0))\\\n",
    "                                     .otherwise(F.lit(0)))\n",
    "\n",
    "####################################################\n",
    "## death\n",
    "## Now use the \"death_after_positive\" column\n",
    "#####################################################\n",
    "## general death, unrelated to covid-19 infection\n",
    "## not used now\n",
    "# covid_pats = covid_pats.withColumn(\"death\",\\\n",
    "#                                      when((col(\"who_score_max\") == 8), \"yes\")\\\n",
    "#                                      .when(col(\"who_score_max\").isNull(), \"no\")\\\n",
    "#                                      .otherwise(\"no\"))\n",
    "\n",
    "covid_pats = covid_pats.withColumn(\"death_after_positive\",\\\n",
    "                                     when( ( (col(\"results\") == \"Positive\") &\\\n",
    "                                            ( ( (col(\"date_of_death\")>=col(\"covidPositive_first_contact_date\")) & (col(\"date_of_death\")<= F.date_add(col(\"covidPositive_first_contact_date\"), 30) ) )\\\n",
    "                                             | ( (col(\"who_score_max\") == 8) &\\\n",
    "                                            (col(\"max_who_date\")>=col(\"covidPositive_first_contact_date\")) & (col(\"max_who_date\")<= F.date_add(col(\"covidPositive_first_contact_date\"), 30) ) ) )\\\n",
    "                                           ), F.lit(1))\\\n",
    "                                     .when(col(\"who_score_max\").isNull(), F.lit(0))\\\n",
    "                                     .otherwise(F.lit(0)))\n",
    "\n",
    "## select some columns and save df\n",
    "## Old code, need cleanup\n",
    "# cols = ['pat_id', 'patient_id', 'age', 'age_range', 'sex', 'ethnicity', 'race1', 'race_v2', 'ethnicity_race','first_contact_date','covidPositive_first_contact_date', 'admission_dt', 'discharge_dt', 'results', 'who_score_max', 'days_to_max_who_score', 'days_hospitalized', 'hospitalized', 'invasive_mechanical_vent', 'death', 'encounter_duration']\n",
    "\n",
    "cols = ['pat_id', 'patient_id', 'age', 'age_range', 'sex', 'ethnicity', 'race1', 'race_v2', 'ethnicity_race','first_contact_date','covidPositive_first_contact_date', 'decided_index_date', 'admission_dt', 'discharge_dt', 'results', 'who_score_max', 'days_to_max_who_score', 'days_hospitalized', 'hospitalized_after_positive', 'IMV_after_positive', 'death_after_positive', 'encounter_duration']\n",
    "covid_pats = covid_pats.select(*cols)\n",
    "\n",
    "# print('all COVID-19 cases : %s' % covid_pats.count())\n",
    "# covid_pats.limit(10).toPandas()\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS rdp_phi_sandbox.qw_all_age_covid_patients_{}_wRaceEthnicity\".format(file_date))\n",
    "covid_pats.write.saveAsTable(\"rdp_phi_sandbox.qw_all_age_covid_patients_{}_wRaceEthnicity\".format(file_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cbbf6ab-0571-43be-b617-6b9fb33ca0d7",
     "showTitle": true,
     "title": "Load needed python libraries (Notice: Check file path)"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run\n",
    "\"/Users/qi.wei1@providence.org/working_folder/IMIDs-COVID19-projects/related_libraries/Python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35235206-67bf-4991-9db5-1ed46d0f0478",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter out patient whose first encounter is for covid test, add bmi feature\n",
    "*  all patients including those first encounter is only covid related, aka we don't have medication history\n",
    "*  add BMI column in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9725342-fa43-4f1c-addd-058843db2ca1",
     "showTitle": true,
     "title": "Function to convert height from ft into meters & do aggregation"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Column\n",
    "# from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "## Input: height in ft unit\n",
    "def convert_height(height):\n",
    "  import re\n",
    "  if height is None:\n",
    "    return None\n",
    "  else:\n",
    "    ftin = re.findall(r\"\\d\", height)\n",
    "    inch = 12*int(ftin[0]) + int(ftin[1])\n",
    "    meter = inch * 0.0254\n",
    "    return meter\n",
    "\n",
    "convert_height_udf = F.udf(lambda height: convert_height(height), DoubleType())\n",
    "\n",
    "def aggregate_data(df, partition_columns, aggregation_columns, order_by=None):\n",
    "  \"\"\"Aggregate data over specified partition columns\n",
    "  \n",
    "  Parameters:\n",
    "  df (PySpark): Dataframe to aggregate\n",
    "  partition_columns (str or list): Field(s) in df on which to partition. If partitioning on only one\n",
    "                                   column, the column name can be provided as a str rather than a\n",
    "                                   list\n",
    "  aggregation_columns (dict): Must be a dict where the keys are fields in df to aggregate and values\n",
    "                              are either a str or list, specifying the aggregation functions to use.\n",
    "                              If using only one aggregation function for a given field, the name of\n",
    "                              the aggregation function can be provided as a str rather than a list.\n",
    "                              A separate column will be added for each aggregation function.\n",
    "  order_by (str or list): Field(s) in df to use for ordering records in each partition. If None, do\n",
    "                          not order. If ordering on only one column, the column name can be provided\n",
    "                          as a str rather than a list\n",
    "  \n",
    "  Result:\n",
    "  PySpark df: Dataframe containing the aggregated results\n",
    "  \n",
    "  \"\"\"\n",
    "  # First argument must be a PySpark dataframe\n",
    "  assert(isinstance(df, DataFrame))\n",
    "  \n",
    "  # Input dataframe must contain specified partition columns\n",
    "  partition_columns = partition_columns if isinstance(partition_columns, list) else [partition_columns]\n",
    "  assert(all([s in df.columns for s in partition_columns]))\n",
    "    \n",
    "  # Perform validity checks on aggregation_columns\n",
    "  assert(isinstance(aggregation_columns, dict))\n",
    "  assert(all([s in df.columns for s in list(aggregation_columns.keys())]))\n",
    "  valid_agg_functions = ['avg', 'collect_list', 'collect_set', 'concat_ws', 'count', 'first', 'last', 'max', 'mean', 'median', 'min', 'stddev', 'sum']\n",
    "  for k in list(aggregation_columns.keys()):\n",
    "    v = aggregation_columns[k]\n",
    "    aggregation_columns[k] = v if isinstance(v, list) else [v]\n",
    "    assert(all([s in valid_agg_functions for s in aggregation_columns[k]]))\n",
    "  \n",
    "  # order_by (if not None) must contain valid column names\n",
    "  if(not(order_by is None)):\n",
    "    order_by = order_by if isinstance(order_by, list) else [order_by]\n",
    "    assert(all([s in df.columns for s in order_by]))\n",
    "  \n",
    "  # Define partition window\n",
    "  w = Window.partitionBy(partition_columns)\n",
    "  if(not(order_by is None)):\n",
    "    w = w.orderBy(order_by).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "  \n",
    "  # Add aggregate columns\n",
    "  results_df = df; new_columns = []\n",
    "  for col, agg_func in aggregation_columns.items():\n",
    "    for s in agg_func:\n",
    "      # Check for boolean field (must be converted to 0/1 for some aggregation functions)\n",
    "      bool_type = (dict(results_df.dtypes)[col] == 'boolean')\n",
    "      \n",
    "      # Apply aggregation function\n",
    "      col_name = '_'.join([col, s])\n",
    "      new_columns = new_columns + [col_name]\n",
    "      print(\"Adding new column '{}'...\".format(col_name))\n",
    "      if(s in ['avg', 'mean']):\n",
    "        if(bool_type):\n",
    "          print(\"Casting boolean column '{}' to integer to calculate avg/mean...\".format(col))\n",
    "          results_df = results_df.withColumn(col_name, F.avg(F.col(col).cast(IntegerType())).over(w))\n",
    "        else:\n",
    "          results_df = results_df.withColumn(col_name, F.avg(col).over(w))\n",
    "      elif(s == 'collect_list'):\n",
    "        results_df = results_df.withColumn(col_name, F.collect_list(col).over(w))\n",
    "      elif(s == 'collect_set'):\n",
    "        results_df = results_df.withColumn(col_name, F.collect_set(col).over(w))\n",
    "      elif(s == 'concat_ws'):\n",
    "        results_df = results_df.withColumn(col_name, F.concat_ws(';', F.collect_list(col).over(w)))\n",
    "      elif(s == 'count'):\n",
    "        results_df = results_df.withColumn(col_name, F.count(col).over(w))\n",
    "      elif(s == 'first'):\n",
    "        results_df = results_df.withColumn(col_name, F.first(col).over(w))\n",
    "      elif(s == 'last'):\n",
    "        results_df = results_df.withColumn(col_name, F.last(col, ignorenulls = True).over(w))\n",
    "      elif(s == 'max'):\n",
    "        results_df = results_df.withColumn(col_name, F.max(col).over(w))\n",
    "      elif(s == 'min'):\n",
    "        results_df = results_df.withColumn(col_name, F.min(col).over(w))\n",
    "      elif(s == 'median'):\n",
    "        results_df = results_df.withColumn(col_name, median_udf(F.collect_list(col).over(w)))\n",
    "      elif(s == 'stddev'):\n",
    "        if(bool_type):\n",
    "          print(\"Casting boolean column '{}' to integer to calculate stddev...\".format(col))\n",
    "          results_df = results_df.withColumn(col_name,\n",
    "                                             F.stddev(F.col(col).cast(IntegerType())).over(w))\n",
    "        else:\n",
    "          results_df = results_df.withColumn(col_name, F.stddev(col).over(w))\n",
    "      elif(s == 'sum'):\n",
    "        if(bool_type):\n",
    "          print(\"Casting boolean column '{}' to integer to calculate sum...\".format(col))\n",
    "          results_df = results_df.withColumn(col_name, F.sum(F.col(col).cast(IntegerType())).over(w))\n",
    "        else:\n",
    "          results_df = results_df.withColumn(col_name, F.sum(col).over(w))\n",
    "  \n",
    "  # Process the final dataframe for return\n",
    "  final_columns = partition_columns + new_columns\n",
    "  results_df = results_df.select(final_columns).dropDuplicates()\n",
    "    \n",
    "  return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b7dc601-246f-4c26-8328-7bf64a6933e7",
     "showTitle": true,
     "title": "Generate table that needed for the next step, Can resume from this checkpoint"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Adding new column &#39;e_weight_last&#39;...\n",
       "Adding new column &#39;e_height_last&#39;...\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Adding new column &#39;e_weight_last&#39;...\nAdding new column &#39;e_height_last&#39;...\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "#########################################################\n",
    "# Generate table that needed for step 4\n",
    "########################################################\n",
    "enc_field = \"pat_id, instance, contact_date, weight, height\"\n",
    "enc_query = \"SELECT %s FROM rdp_phi.encounter WHERE instance ='1000'\" %enc_field\n",
    "df_enc = spark.sql(enc_query) \n",
    "\n",
    "## get adult_covid df\n",
    "covid_pats = spark.sql(\"\"\"SELECT * FROM rdp_phi_sandbox.qw_all_age_covid_patients_{}_wRaceEthnicity\"\"\".format(file_date))\n",
    "\n",
    "## only select needed cols\n",
    "cols = ['pat_id', 'patient_id', 'age', 'age_range', 'sex', 'ethnicity', 'race1', 'race_v2', 'ethnicity_race', 'results', 'hospitalized_after_positive', 'IMV_after_positive', 'death_after_positive', 'covidPositive_first_contact_date', 'decided_index_date']  \n",
    "\n",
    "covid_pats = covid_pats.select(*cols).dropDuplicates()\n",
    "\n",
    "## inner join df_enc + covid_pats\n",
    "temp_join_df = df_enc.join(covid_pats, on = [\"pat_id\"], how = \"inner\") \\\n",
    "                    .withColumn('e_weight', F.col('weight')*0.0283495) \\\n",
    "                    .withColumn('e_height', convert_height_udf(F.col('height'))) \\\n",
    "                    .drop('weight', 'height')\n",
    "\n",
    "temp_join_df = temp_join_df.orderBy(\"contact_date\")\n",
    "partition_by = ['pat_id','instance']\n",
    "aggregate_by = {'e_weight' : 'last', \n",
    "                'e_height' : 'last'}\n",
    "\n",
    "agg_df = aggregate_data(temp_join_df, partition_columns = partition_by , aggregation_columns = aggregate_by)\n",
    "agg_df = agg_df.withColumnRenamed('e_weight_last', 'weight')\\\n",
    "               .withColumnRenamed('e_height_last', 'height')\\\n",
    "               .withColumn('BMI', F.col('weight')/F.col('height')**2)\n",
    "\n",
    "## Fill the null values (around 6%) to be all median values\n",
    "## use the imputer function\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols = ['BMI'],\n",
    "    outputCols = [\"{}_imputed\".format(a) for a in ['BMI']]\n",
    ").setStrategy(\"median\")\n",
    "\n",
    "agg_impute_df = imputer.fit(agg_df).transform(agg_df)\n",
    "\n",
    "## Notice!!!\n",
    "## Temp comment out below line just to get the missingness on BMI\n",
    "agg_impute_df = agg_impute_df.drop(\"BMI\").withColumnRenamed(\"BMI_imputed\", \"BMI\")\n",
    "\n",
    "## left join adult_covid + agg_impute_df\n",
    "df_enc_covid = temp_join_df.join(agg_impute_df, on = [\"pat_id\"], how = 'left')\n",
    "\n",
    "## Use decided_index_date or first_contact_date?\n",
    "df_enc_covid = df_enc_covid.withColumn(\"contactdate_before_covid19date\", when((col(\"contact_date\")) < (col(\"decided_index_date\")), (col(\"contact_date\"))))\n",
    "      \n",
    "\n",
    "cols = ['pat_id', 'patient_id', 'age', 'age_range', 'sex', 'ethnicity', 'race1', 'race_v2', 'ethnicity_race', 'BMI', 'results', 'hospitalized_after_positive', 'IMV_after_positive', 'death_after_positive', 'contactdate_before_covid19date', 'covidPositive_first_contact_date', 'decided_index_date']\n",
    "\n",
    "## Only select those cols we want\n",
    "df_enc_covid = df_enc_covid.select(*cols).dropDuplicates()\n",
    "\n",
    "# table_name = \"rdp_phi_sandbox.qw_all_age_covid_patients_{}_wEncounter_wRaceEthnicity\".format(file_date)\n",
    "# spark.sql(\"DROP TABLE IF EXISTS rdp_phi_sandbox.qw_all_age_covid_patients_{}_wEncounter_wRaceEthnicity\".format(file_date))\n",
    "# df_enc_covid.write.saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c681c0b-5c28-476d-9c45-b16a3975625c",
     "showTitle": true,
     "title": "Can resume here"
    }
   },
   "outputs": [],
   "source": [
    "# table_name = \"rdp_phi_sandbox.qw_all_age_covid_patients_{}_wEncounter_wRaceEthnicity\".format(file_date)\n",
    "# df_enc_covid = spark.sql(\"\"\"SELECT * from rdp_phi_sandbox.qw_all_age_covid_patients_{}_wEncounter_wRaceEthnicity\"\"\".format(file_date))\n",
    "\n",
    "# ## check the df\n",
    "# # display(df_enc_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14e1d970-15b1-4966-bc33-046ad80eae12",
     "showTitle": true,
     "title": "Check how many patients have first contact just due to COVID (those with no contactdate_before_covid19date)"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark.sql.functions import col,isnan,when,count\n",
    "# search_col=[\"contactdate_before_covid19date\"]\n",
    "# # print(\"The total number of patient+encounter records are: \", df_enc_covid.count())\n",
    "\n",
    "# # print(\"The encounter number of patients have first contact just due to COVID (those with no contactdate_before_covid19date) are: \")\n",
    "# # df_enc_covid.select([count(when(col(c).isNull(), c)).alias(c) for c in search_col]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e009fda7-de90-4030-9021-057d7e83b4e3",
     "showTitle": true,
     "title": "Filter based on contactdate_before_covid19date, eliminate all Null rows"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">The current encounter number of patients have first contact just due to COVID (those with no contactdate_before_covid19date) are: \n",
       "+------------------------------+\n",
       "contactdate_before_covid19date|\n",
       "+------------------------------+\n",
       "                             0|\n",
       "+------------------------------+\n",
       "\n",
       "The current number of unique patients after filtering out first time COVID patients is: \n",
       "Out[9]: 1516136</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">The current encounter number of patients have first contact just due to COVID (those with no contactdate_before_covid19date) are: \n+------------------------------+\n|contactdate_before_covid19date|\n+------------------------------+\n|                             0|\n+------------------------------+\n\nThe current number of unique patients after filtering out first time COVID patients is: \nOut[9]: 1516136</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "search_col=[\"contactdate_before_covid19date\"]\n",
    "\n",
    "df_enc_covid_recurrent_patient_only = df_enc_covid.na.drop(subset=[\"contactdate_before_covid19date\"])\n",
    "# print(\"The total number of patient+encounter records now is: \", df_enc_covid_recurrent_patient_only.count())\n",
    "\n",
    "########################################################################\n",
    "## Notice! Sanity check, notice this table should have 0 rows count now\n",
    "########################################################################\n",
    "print(\"The current encounter number of patients have first contact just due to COVID (those with no contactdate_before_covid19date) are: \")\n",
    "df_enc_covid_recurrent_patient_only.select([count(when(col(c).isNull(), c)).alias(c) for c in search_col]).show()\n",
    "\n",
    "print(\"The current number of unique patients after filtering out first time COVID patients is: \")\n",
    "\n",
    "df_enc_covid_recurrent_patient_only.select(\"pat_id\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c219b977-48d1-4430-84cd-3a3e239b1948",
     "showTitle": true,
     "title": "Get the missingness statistic for BMI"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Missing tested pts:\n",
       "66718\n",
       "Missing positive pts:\n",
       "9384\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Missing tested pts:\n66718\nMissing positive pts:\n9384\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # print(\"Missing rate:\")\n",
    "# # print(agg_impute_df.select(col('BMI')).filter(col('BMI').isNull()).count()/agg_impute_df.count()*100)\n",
    "\n",
    "# ## Only positive or negative patients\n",
    "# df_BMI_pos_neg = df_enc_covid_recurrent_patient_only.filter( (df_enc_covid_recurrent_patient_only.results == \"Positive\") | (df_enc_covid_recurrent_patient_only.results == \"Negative\") )\n",
    "\n",
    "# ## Only positive\n",
    "# df_BMI_pos = df_enc_covid_recurrent_patient_only.filter( (df_enc_covid_recurrent_patient_only.results == \"Positive\"))\n",
    "\n",
    "# print(\"Missing tested pts:\")\n",
    "# print(df_BMI_pos_neg.select('pat_id', 'BMI').filter(col('BMI').isNull()).select('pat_id').distinct().count())\n",
    "\n",
    "# print(\"Missing positive pts:\")\n",
    "# print(df_BMI_pos.select('pat_id', 'BMI').filter(col('BMI').isNull()).select('pat_id').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00bb46f4-1258-46b4-ac0b-ecba280e11c3",
     "showTitle": true,
     "title": "Check the df before write table"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_enc_covid_recurrent_patient_only.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7919c0da-96ee-4cc7-a376-028d678bf3ce",
     "showTitle": true,
     "title": "Save the filtered encounter table with only recurrent patients"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[609]: DataFrame[]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[609]: DataFrame[]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_name = \"rdp_phi_sandbox.qw_all_age_covid_patients_{}_wEncounter_wRaceEthnicity_nOneTimeCovidPatient_wFirstPosDate\".format(file_date)\n",
    "spark.sql(\"DROP TABLE IF EXISTS rdp_phi_sandbox.qw_all_age_covid_patients_{}_wEncounter_wRaceEthnicity_nOneTimeCovidPatient_wFirstPosDate\".format(file_date))\n",
    "\n",
    "## Need to refresh the table to avoid issue\n",
    "df_enc_covid_recurrent_patient_only.write.saveAsTable(table_name)\n",
    "\n",
    "spark.sql(\"REFRESH TABLE rdp_phi_sandbox.qw_all_age_covid_patients_{}_wEncounter_wRaceEthnicity_nOneTimeCovidPatient_wFirstPosDate\".format(file_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9695a1c8-1587-4e8f-8ae4-407e29286141",
     "showTitle": true,
     "title": "Counts of recurrent patients (no first time patient due to COVID)"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_enc_covid_recurrent_patient_only.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81bcb6af-df42-4251-aae7-1ca288e2e61a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task completed, continue to step 2\n",
    "#### /Users/jennifer.hadlock2@providence.org/GreenerGrass/Qi/Transfer-Qi/COVID-IBD-IMIDs-paper/IMIDs_working_in_progress/Step2_generate_IBD_OtherIMIDs_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83096499-6713-4c5e-be1d-90b3dd451998",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Task completed, continue to step 2\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Task completed, continue to step 2\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################################################\n",
    "# The following code are for additional analysis\n",
    "######################################################\n",
    "# print(\"Total number of records in the qw_all_age_covid_patients_{}_wEncounter_wRaceEthnicity table: \".format(file_date), df_enc_covid.count())\n",
    "print(\"Task completed, continue to step 2\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Step1.1_Collect_COVID19_Tests_results",
   "notebookOrigID": 1383230928368363,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
